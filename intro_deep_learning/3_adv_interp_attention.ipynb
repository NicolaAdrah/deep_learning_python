{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: For each section I put some explanation for me to study the theory, so you can skip it.**\n",
        "\n",
        "## 1. FGSM Attack\n",
        "The Fast Gradient Sign Method (FGSM) is one of the simplest methods to generate adversarial examples. The idea is to perturb an input image $x$ in the direction of the gradient of the loss $J$ with respect to $x$ so that the model's prediction is maximally affected. Mathematically, the adversarial example $x_{\\text{adv}}$ is generated by:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "x_{\\text{adv}} = x + \\epsilon \\cdot \\text{sign} (∇_x J(x,y))\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "* $\\epsilon$ controls the magnitude of the perturbation.\n",
        "* The gradient $∇_x J(x,y)$ shows which direction in pixel space would most increase the loss.\n",
        "* The sign operation ensures that the perturbation is in a fixed direction (either +1 or –1 for each pixel)."
      ],
      "metadata": {
        "id": "W8Gxdq-43TXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "hwa55_ni5DPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "transform_dict = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "sZXYzZg85GY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_dict['train'])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_dict['test'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sI6C-Tln8c7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "        self.pool = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.head = nn.Linear(128*7*7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        self.last_feature_map = x\n",
        "        if self.last_feature_map.requires_grad:\n",
        "            self.last_feature_map.retain_grad()\n",
        "        x = self.pool(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "ZHOUsM4A87JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "dcz68LFY9Bkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions to get accuracy\n",
        "def get_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            out = model(x)\n",
        "            correct += (torch.argmax(out, dim=1) == y).sum().item()\n",
        "    return correct / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "Ir_zstio9DYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Class-based Adversarial Attacks and GradCAM\n",
        "# ----------------------------\n",
        "\n",
        "# FGSM Attack (Class-based implementation)\n",
        "class FGSMAttack:\n",
        "    def __init__(self, model, epsilon, loss_fn=None):\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.loss_fn = loss_fn if loss_fn is not None else nn.CrossEntropyLoss()\n",
        "\n",
        "    def perturb(self, x, y):\n",
        "        x_adv = x.clone().detach().requires_grad_(True)\n",
        "        outputs = self.model(x_adv)\n",
        "        loss = self.loss_fn(outputs, y)\n",
        "        self.model.zero_grad()\n",
        "        loss.backward()\n",
        "        grad_sign = x_adv.grad.data.sign()\n",
        "        x_adv = x_adv + self.epsilon * grad_sign\n",
        "        x_adv = torch.clamp(x_adv, 0, 1)\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "SHOOV37I9HVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. BIM Attack\n",
        "\n",
        "The Basic Iterative Method (BIM) (also known as Iterative FGSM) extends FGSM by applying the perturbation multiple times with a smaller step size $\\alpha$. After each iteration, the perturbation is clipped so that the total perturbation remains within the $\\epsilon$-ball around the original input. Mathematically, for iteration $i$:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "x_{\\text{adv}}^{i+1} = \\text{clip}_{x,\\epsilon} ( x_{\\text{adv}}^{i} + \\alpha \\cdot (∇_{x_{\\text{adv}}^{i}} J(x_{\\text{adv}}^{i}, y))\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "* $\\alpha$ is the step size (often set to a small value such as $\\epsilon/N$ for $N$ iterations).\n",
        "* Clipping ensures that $||x_{\\text{adv}} - x||_{\\infty} \\leq \\epsilon$."
      ],
      "metadata": {
        "id": "NEGz-aEy9ZEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BIM Attack (Iterative FGSM)\n",
        "class BIMAttack:\n",
        "    def __init__(self, model, epsilon, alpha, iters, loss_fn=None):\n",
        "        self.model = model\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.iters = iters\n",
        "        self.loss_fn = loss_fn if loss_fn is not None else nn.CrossEntropyLoss()\n",
        "\n",
        "    def perturb(self, x, y):\n",
        "        x_adv = x.clone().detach()\n",
        "        for _ in range(self.iters):\n",
        "            x_adv.requires_grad = True\n",
        "            outputs = self.model(x_adv)\n",
        "            loss = self.loss_fn(outputs, y)\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad_sign = x_adv.grad.data.sign()\n",
        "            x_adv = x_adv + self.alpha * grad_sign\n",
        "            # Clip to ensure the overall perturbation stays within [-epsilon, epsilon]\n",
        "            perturbation = torch.clamp(x_adv - x, min=-self.epsilon, max=self.epsilon)\n",
        "            x_adv = torch.clamp(x + perturbation, 0, 1).detach()\n",
        "        return x_adv"
      ],
      "metadata": {
        "id": "ilzgClCm9KZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GradCAM Visualization\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate(self, input_image, target_class):\n",
        "        self.model.zero_grad()\n",
        "        output = self.model(input_image)\n",
        "        loss = output[0, target_class]\n",
        "        loss.backward(retain_graph=True)\n",
        "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
        "        grad_cam_map = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
        "        grad_cam_map = F.relu(grad_cam_map)\n",
        "        grad_cam_map = F.interpolate(grad_cam_map, size=input_image.shape[2:], mode='bilinear', align_corners=False)\n",
        "        grad_cam_map = (grad_cam_map - grad_cam_map.min()) / (grad_cam_map.max() - grad_cam_map.min() + 1e-8)\n",
        "        return grad_cam_map"
      ],
      "metadata": {
        "id": "cxXHE_PN9X3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Integration Examples\n",
        "# ----------------------------\n",
        "\n",
        "# 1. Using FGSM for evaluation (replacing your fgsm_attack function)\n",
        "fgsm_attack_instance = FGSMAttack(model, epsilon=8/255)\n",
        "def get_adversarial_accuracy(model, dataloader, attack, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # Use the attack instance's perturb method\n",
        "        adv = attack.perturb(x, y)\n",
        "        with torch.no_grad():\n",
        "            out = model(adv)\n",
        "            correct += (torch.argmax(out, dim=1) == y).sum().item()\n",
        "    return correct / len(dataloader.dataset)\n",
        "\n",
        "adv_acc = get_adversarial_accuracy(model, testloader, fgsm_attack_instance, device)\n",
        "print(\"Adversarial Accuracy (FGSM):\", adv_acc)\n",
        "\n",
        "# 2. Using BIM for evaluation and adversarial training\n",
        "bim_attack_instance = BIMAttack(model, epsilon=8/255, alpha=2/255, iters=5)\n",
        "\n",
        "# Example: Evaluate BIM adversarial accuracy\n",
        "adv_acc_bim = get_adversarial_accuracy(model, testloader, bim_attack_instance, device)\n",
        "print(\"Adversarial Accuracy (BIM):\", adv_acc_bim)\n",
        "\n",
        "# 3. GradCAM visualization on a clean image and an adversarial image\n",
        "# For GradCAM, we choose the last convolutional layer in your CNN.\n",
        "# In your CNN, the last conv layer is at index 4 in model.conv.\n",
        "gradcam = GradCAM(model, target_layer=model.conv[4])\n",
        "x_batch, y_batch = next(iter(testloader))\n",
        "x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "# Generate adversarial image using FGSM for visualization example\n",
        "adv_image = fgsm_attack_instance.perturb(x_batch, y_batch)\n",
        "# Use the true label for the clean image and adversarial label for the adversarial image\n",
        "clean_label = y_batch[0].item()\n",
        "adv_label = model(adv_image).argmax(dim=1)[0].item()\n",
        "\n",
        "# Generate heatmaps\n",
        "heatmap_clean = gradcam.generate(x_batch[0:1], clean_label)\n",
        "heatmap_adv = gradcam.generate(adv_image[0:1], adv_label)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axes[0].imshow(x_batch[0].detach().cpu().permute(1, 2, 0))\n",
        "axes[0].set_title(f\"Clean Image: {testset.classes[clean_label]}\")\n",
        "axes[1].imshow(adv_image[0].detach().cpu().permute(1, 2, 0))\n",
        "axes[1].set_title(f\"Adversarial Image: {testset.classes[adv_label]}\")\n",
        "axes[2].imshow(heatmap_adv[0,0].detach().cpu(), cmap='jet')\n",
        "axes[2].set_title(\"GradCAM on Adv Image\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Adversarial Training with BIM\n",
        "def adversarial_training(model, train_loader, optimizer, criterion, device, attack, epochs):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_clean = 0\n",
        "        correct_adv = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # Generate adversarial examples using the provided attack instance\n",
        "            images_adv = attack.perturb(images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs_clean = model(images)\n",
        "            outputs_adv = model(images_adv)\n",
        "            loss_clean = criterion(outputs_clean, labels)\n",
        "            loss_adv = criterion(outputs_adv, labels)\n",
        "            loss_total = loss_clean + loss_adv\n",
        "            loss_total.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss_total.item()\n",
        "            _, pred_clean = outputs_clean.max(1)\n",
        "            _, pred_adv = outputs_adv.max(1)\n",
        "            correct_clean += (pred_clean == labels).sum().item()\n",
        "            correct_adv += (pred_adv == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} Loss: {running_loss/len(train_loader):.4f} \"\n",
        "              f\"Clean Acc: {correct_clean/total:.4f} Adv Acc: {correct_adv/total:.4f}\")\n",
        "\n",
        "# Reinitialize or load your model and optimizer as needed before adversarial training\n",
        "model = CNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 10\n",
        "print(\"Adversarial Training with BIM Attack\")\n",
        "adversarial_training(model, trainloader, optimizer, criterion, device, bim_attack_instance, epochs)\n",
        "\n",
        "print(\"Post-training Clean Accuracy:\", get_accuracy(model, testloader, device))\n",
        "print(\"Post-training Adv Accuracy (using FGSM):\", get_adversarial_accuracy(model, testloader, fgsm_attack_instance, device))"
      ],
      "metadata": {
        "id": "_U3Kc0Px_q2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obd03HUj_ur5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}